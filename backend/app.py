# app.py
import os, base64, time, threading
from time import monotonic, sleep
from pathlib import Path

import cv2
import numpy as np
from flask import Flask, request
from flask_socketio import SocketIO, emit
from loguru import logger
from dotenv import load_dotenv

# ===== ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ø¨ÙŠØ¦Ø© =====
BASE_DIR = Path(__file__).resolve().parent
FRONT_DIR = BASE_DIR.parent / "frontend"
ASSETS_DIR = FRONT_DIR / "assets"

load_dotenv(BASE_DIR / ".env")

app = Flask(__name__)
app.config["SECRET_KEY"] = "rashid_kiosk_2025"
socketio = SocketIO(app, cors_allowed_origins="*", async_mode="threading", ping_timeout=60, ping_interval=25)

# ===== Ø­Ø§Ù„Ø© Ø§Ù„Ø¬Ù„Ø³Ø© =====
face_present = False          # Ù‡Ù„ ÙÙŠ Ø´Ø®Øµ Ù‚Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§
engaged = False               # Ù‚Ù„Ù†Ø§ Ù„Ù‡ Ø§Ù„ØªØ±Ø­ÙŠØ¨ ÙˆÙ„Ø§ Ø¨Ø¹Ø¯
is_speaking = False           # Ø±Ø§Ø´Ø¯ ÙŠØªÙƒÙ„Ù… Ø§Ù„Ø¢Ù†ØŸ
last_reply_time = 0.0         # Ø­Ø§Ø¬Ø² Ø§Ù„Ø¥ÙŠÙƒÙˆ

# NEW: Ù…Ø®Ø²Ù† Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„
SESSIONS = {}  # key = request.sid -> {"history": [...], "slots": {...}, "pending_question": None}

# ===== Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Ø£ÙØ§ØªØ§Ø±) =====
video_frames = {"silent": [], "speaking": [], "listening" : [] }
default_frame = None
frame_idx = 0

SILENT_MP4   = ASSETS_DIR / "rashid_silent.mp4"
SPEAKING_MP4 = ASSETS_DIR / "rashid_speaking.mp4"
LISTENING_MP4 = ASSETS_DIR / "rashid_listening.mp4"
FALLBACK_PNG = ASSETS_DIR / "avatar.png"


def _create_default_frame():
    global default_frame
    # ØµÙˆØ±Ø© Ø³ÙˆØ¯Ø§Ø¡ Ù…Ø¹ ÙƒØªØ§Ø¨Ø©ØŒ ÙˆÙ„Ùˆ ÙÙŠÙ‡ avatar.png Ù†Ø³ØªØ®Ø¯Ù…Ù‡
    if FALLBACK_PNG.exists():
        img = cv2.imread(str(FALLBACK_PNG))
        if img is not None:
            default_frame = cv2.resize(img, (1280, 720))
            return
    default_frame = np.zeros((720, 1280, 3), dtype=np.uint8)
    cv2.putText(default_frame, "Rashid Assistant", (380, 370),
                cv2.FONT_HERSHEY_SIMPLEX, 2.0, (255, 255, 255), 3)


def _load_video(path: Path, key: str):
    if not path.exists():
        return
    cap = cv2.VideoCapture(str(path))
    if not cap.isOpened():
        return
    while True:
        ok, frame = cap.read()
        if not ok:
            break
        frame = cv2.resize(frame, (1280, 720))
        video_frames[key].append(frame)
    cap.release()
    logger.info(f"âœ… {key} frames: {len(video_frames[key])}")


def load_video_frames():
    logger.info("ğŸ¬ Loading avatar frames...")
    try:
        _load_video(SILENT_MP4, "silent")
        _load_video(SPEAKING_MP4, "speaking")
        if not video_frames["silent"] and not video_frames["speaking"]:
            _create_default_frame()
    except Exception as e:
        logger.error(f"âŒ load_video_frames: {e}")
        _create_default_frame()


def get_current_frame():
    global frame_idx
    try:
        if is_speaking and video_frames["speaking"]:
            f = video_frames["speaking"][frame_idx % len(video_frames["speaking"])]
        elif video_frames["silent"]:
            f = video_frames["silent"][frame_idx % len(video_frames["silent"])]
        else:
            f = default_frame
        frame_idx += 1
        return f
    except Exception as e:
        logger.error(f"âŒ get_current_frame: {e}")
        return default_frame


def frame_to_b64(frame):
    try:
        _, buf = cv2.imencode(".jpg", frame)
        return base64.b64encode(buf).decode("utf-8")
    except Exception as e:
        logger.error(f"âŒ frame_to_b64: {e}")
        return None


def video_loop():
    while True:
        try:
            bgr = get_current_frame()
            b64 = frame_to_b64(bgr)
            if b64:
                socketio.emit("video_frame", {"frame": b64})
            sleep(1/30)
        except Exception as e:
            logger.error(f"âŒ video_loop: {e}")
            sleep(0.5)

# ===== Ø§Ù„ØªØ­ÙƒÙ… Ø¨Ø­Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ø§Ù… Ù„Ù…Ù†Ø¹ Ø§Ù„Ø¥ÙŠÙƒÙˆ =====
def set_speaking(flag: bool):
    global is_speaking, last_reply_time
    is_speaking = bool(flag)
    socketio.emit("speak_state", {"speaking": is_speaking})
    if is_speaking:
        last_reply_time = monotonic()


def say(text: str):
    """
    ÙŠØ±Ø³Ù„ Ø±Ø¯Ù‘ Ù†ØµÙŠ ÙˆØµÙˆØªÙŠ + ÙŠØ¶Ø¨Ø· speaking True Ù„ÙØªØ±Ø© Ù‚ØµÙŠØ±Ø© Ø­ØªÙ‰ Ù…Ø§ ÙŠØ³Ù…Ø¹ Ù†ÙØ³Ù‡.
    """
    set_speaking(True)
    socketio.emit("server_response", {"data": text})
    socketio.emit("voice_response", {"text": text})

    # Ø®ÙÙ‘Ø¶ speaking Ø¨Ø¹Ø¯ Ø´ÙˆÙŠ (ÙŠÙØ¹Ø·ÙŠ ÙˆÙ‚Øª Ù„Ù„Ù…ØªØµÙØ­ ÙŠÙˆÙ‚Ù Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹)
    def _reset():
        sleep(3.5)
        set_speaking(False)
    threading.Thread(target=_reset, daemon=True).start()

# ===== LLM =====
from llm import smart_answer  # ÙŠÙØ±Ø¬Ø¹ dict ÙÙŠÙ‡ {text, mode, sources}

# ===== Socket.IO =====
@socketio.on('connect')
def on_connect():
    logger.info("ğŸ”Œ Client connected")
    emit('connection_status', {'status': 'connected'})


@socketio.on("disconnect")
def on_disconnect():
    logger.info("ğŸ”Œ Client disconnected")

@socketio.on('start_stream')
def on_start_stream():
    frame = get_current_frame()
    b64 = frame_to_b64(frame)
    if b64:
        emit('video_frame', {'frame': b64})

@socketio.on('voice_input')
def on_voice_input(data):
    user_text = (data or {}).get("text", "").strip()
    if not user_text:
        return
    handle_user_text(user_text)

@socketio.on('user_text')
def on_user_text(data):
    text = (data or {}).get("text","").strip()
    if not text:
        return
    logger.info(f"ğŸ¤ User: {text}")
    try:
        out = smart_answer(text)          # out dict
        reply_text = out.get("text","")
    except Exception:
        logger.exception("smart_answer failed")
        reply_text = "ØµØ§Ø±Øª Ù…Ø´ÙƒÙ„Ø© Ø¨Ø³ÙŠØ·Ø© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯. Ø¬Ø±Ù‘Ø¨ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¤Ø§Ù„."

    # ğŸ”Š Ø£Ø¹Ù„Ù† Ø£Ù† Ø§Ù„Ø¨ÙˆØª ÙŠØªÙƒÙ„Ù…
    socketio.emit("speak_state", {"speaking": True})
    socketio.emit("voice_response", {"text": reply_text})
    socketio.emit("server_response", {"data": reply_text})

    def reset():
        global is_speaking
        time.sleep(3)
        is_speaking = False
        # â¹ï¸ Ø¨Ø¹Ø¯ Ù…Ø§ ÙŠØ®Ù„Øµ Ø£Ø±Ø¬Ø¹ Ø§Ù„Ø­Ø§Ù„Ø©
        socketio.emit("speak_state", {"speaking": False})
    threading.Thread(target=reset, daemon=True).start()



# ===== ØªØ±Ø­ÙŠØ¨/ØªÙˆØ¯ÙŠØ¹ Ø¨Ø§Ù„Ø­Ø¶ÙˆØ± =====
def greet_on_arrival():
    # ØªØ±Ø­ÙŠØ¨ Ø°ÙƒÙŠ + Ù‚ÙÙ„ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© (speaking=True)
    reply = " Ø­ÙÙŠÙ‘ÙØ§ÙƒÙ Ø§Ù„Ù„Ù‘ÙÙ‡! Ø£ÙÙ†ÙØ§ Ø±ÙØ§Ø´ÙØ¯ØŒ ÙƒÙŠÙ Ø§Ù‚Ø¯Ø± Ø§Ø®Ø¯Ù…ÙƒØŸ"
    socketio.emit("speak_state", {"speaking": True})
    socketio.emit('voice_response', {'text': reply})
    socketio.emit('server_response', {'data': reply})
    # Ø§ÙØªØ­ Ø§Ù„Ù…ÙŠÙƒ Ø¨Ø¹Ø¯ Ù…Ø§ ÙŠØ®Ù„Øµ Ø§Ù„Ù†Ø·Ù‚
    threading.Thread(target=_end_speaking_after, args=(3.0,), daemon=True).start()

def farewell_on_leave():
    reply = "ØªÙØ´ÙØ±Ù‘ÙÙÙ’ØªÙ Ø¨ÙØ®ÙØ¯Ù’Ù…ÙØªÙÙƒÙØŒ Ù…ÙØ¹Ù Ø§Ù„Ø³Ù‘ÙÙ„ÙØ§Ù…ÙØ©!"
    socketio.emit("speak_state", {"speaking": True})
    socketio.emit('voice_response', {'text': reply})
    socketio.emit('server_response', {'data': reply})
    threading.Thread(target=_end_speaking_after, args=(2.0,), daemon=True).start()

def _end_speaking_after(sec: float):
    time.sleep(sec)
    socketio.emit("speak_state", {"speaking": False})

# ========= Ø§Ù„Ø±Ø¯ Ø§Ù„Ø°ÙƒÙŠ =========
def handle_user_text(user_text: str):
    logger.info(f"ğŸ¤ User: {user_text}")
    try:
        out = smart_answer(user_text)   # {'text': ..., ...}
        reply_text = out.get("text", "")
    except Exception:
        logger.exception("smart_answer failed")
        reply_text = "ØµØ§Ø±Øª Ù…Ø´ÙƒÙ„Ø© Ø¨Ø³ÙŠØ·Ø© ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯. Ø¬Ø±Ù‘Ø¨ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¤Ø§Ù„."

    # ğŸ”‡ Ø§Ù‚ÙÙ„ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ù‚Ø¨Ù„ Ù…Ø§ Ù†ØªÙƒÙ„Ù…
    socketio.emit("speak_state", {"speaking": True})
    socketio.emit('voice_response', {'text': reply_text})
    socketio.emit('server_response', {'data': reply_text})
    # ğŸ”Š Ø§ÙØªØ­ Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹ Ø¨Ø¹Ø¯ Ù…Ø§ ÙŠØ®Ù„Øµ Ø§Ù„ØµÙˆØª
    threading.Thread(target=_end_speaking_after, args=(4.0,), daemon=True).start()


# ===== ÙƒØ§Ø´Ù Ø§Ù„ÙˆØ¬Ù‡ (MediaPipe) =====
def face_presence_watcher(cam_index=0, greet_delay_s=2.0, farewell_delay_s=5.0,
                          min_conf=0.6, min_area=0.04):
    """
    ÙŠØ±Ø§Ù‚Ø¨ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§ ÙˆÙŠÙÙØ¹Ù‘Ù„ Ø§Ù„ØªØ±Ø­ÙŠØ¨ Ø¨Ø¹Ø¯ Ø«Ø¨Ø§Øª ÙˆØ¬ÙˆØ¯ Ø§Ù„ÙˆØ¬Ù‡ Ù„Ù…Ø¯Ø© greet_delay_sØŒ
    ÙˆÙŠÙÙØ¹Ù‘Ù„ Ø§Ù„ØªÙˆØ¯ÙŠØ¹ Ø¨Ø¹Ø¯ Ø§Ø®ØªÙØ§Ø¡ Ø§Ù„ÙˆØ¬Ù‡ Ù„Ù…Ø¯Ø© farewell_delay_s.
    """
    global face_present
    try:
        import mediapipe as mp
    except Exception as e:
        logger.error(f"âŒ mediapipe not available: {e}")
        return

    cap = cv2.VideoCapture(cam_index, cv2.CAP_DSHOW)
    if not cap.isOpened():
        logger.error("âŒ Ù„Ø§ Ø£Ø³ØªØ·ÙŠØ¹ ÙØªØ­ Ø§Ù„ÙƒØ§Ù…ÙŠØ±Ø§")
        return

    mp_fd = mp.solutions.face_detection

    # Ø·ÙˆØ§Ø¨Ø¹ Ø²Ù…Ù†ÙŠØ© Ù„Ø¶Ø¨Ø· Ø§Ù„ØªØ£Ø®ÙŠØ±
    first_seen_ts = None         # Ø£ÙˆÙ„ Ù„Ø­Ø¸Ø© Ø¨Ø¯Ø£ ÙÙŠÙ‡Ø§ Ø§Ù„ÙˆØ¬Ù‡ ÙŠØ¸Ù‡Ø± Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§ØµÙ„
    last_seen_ts = None          # Ø¢Ø®Ø± Ù„Ø­Ø¸Ø© ÙƒØ§Ù† ÙÙŠÙ‡Ø§ Ø§Ù„ÙˆØ¬Ù‡ Ø¸Ø§Ù‡Ø±
    greeted = False              # Ù‡Ù„ Ù‚Ø¯ Ø·ÙÙ„Ø¨ Ø§Ù„ØªØ±Ø­ÙŠØ¨ Ù„Ù‡Ø°Ù‡ Ø§Ù„Ø¬Ù„Ø³Ø©ØŸ

    with mp_fd.FaceDetection(model_selection=0, min_detection_confidence=min_conf) as fd:
        while True:
            ok, frame = cap.read()
            if not ok:
                sleep(0.03)
                continue

            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            res = fd.process(rgb)

            # ØªØ­Ù‚Ù‘Ù‚ ÙˆØ¬ÙˆØ¯ ÙˆØ¬Ù‡ Ø¨Ù…Ø³Ø§Ø­Ø© ÙƒØ§ÙÙŠØ©
            has_face = False
            if res and res.detections:
                for det in res.detections:
                    bb = det.location_data.relative_bounding_box
                    if max(bb.width, 0) * max(bb.height, 0) >= min_area:
                        has_face = True
                        break

            now = monotonic()

            if has_face:
                last_seen_ts = now
                if first_seen_ts is None:
                    first_seen_ts = now

                # Ø¥Ø°Ø§ Ù„Ù… Ù†ÙØ±Ø­Ù‘Ø¨ Ø¨Ø¹Ø¯ØŒ ÙˆØ§Ù†ØªØ¸Ù… Ø§Ù„ÙˆØ¬ÙˆØ¯ Ù„Ù…Ø¯Ø© ÙƒØ§ÙÙŠØ© -> Ø±Ø­Ù‘Ø¨
                if not greeted and (now - first_seen_ts) >= greet_delay_s:
                    face_present = True
                    greeted = True
                    socketio.emit("presence", {"present": True})
                    logger.info("ğŸŸ¢ Face present (stable) -> greet")
                    greet_on_arrival()
            else:
                # Ù„Ø§ ÙŠÙˆØ¬Ø¯ ÙˆØ¬Ù‡: ØµÙÙ‘Ø± Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¸Ù‡ÙˆØ±ØŒ ÙˆØ±Ø§Ù‚Ø¨ Ø²Ù…Ù† Ø§Ù„ØºÙŠØ§Ø¨
                first_seen_ts = None
                if last_seen_ts is not None and greeted:
                    if (now - last_seen_ts) >= farewell_delay_s:
                        # ØºÙŠØ§Ø¨ Ù…Ø³ØªÙ‚Ø± Ø¨Ù…Ø§ ÙŠÙƒÙÙŠ -> ÙˆØ¯Ø§Ø¹
                        face_present = False
                        greeted = False
                        last_seen_ts = None
                        socketio.emit("presence", {"present": False})
                        logger.info("âšª Face absent (stable) -> farewell")
                        farewell_on_leave()

            # Ø³Ù„ÙŠØ¨ Ø®ÙÙŠÙ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ù…Ù„ (Ù„Ø§ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø²Ù…Ù† ÙƒÙ…Ø¹ÙŠØ§Ø±)
            sleep(0.03)


# ===== Ø§Ù„ØªØ´ØºÙŠÙ„ =====
if __name__ == "__main__":
    logger.info("ğŸš€ Starting Rashid Kiosk...")
    load_video_frames()
    threading.Thread(target=video_loop, daemon=True).start()
    threading.Thread(
    target=face_presence_watcher,
    args=(0, 2.0, 5.0),  # greet_delay_s=2s, farewell_delay_s=5s
    daemon=True
    ).start()
    logger.info("ğŸŒ Open frontend: frontend/index.html")
    socketio.run(app, host="0.0.0.0", port=5000, debug=False)







@socketio.on("tts_start")
def _tts_start():
    set_speaking(True)

@socketio.on("tts_end")
def _tts_end():
    set_speaking(False)